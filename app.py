from flask import Flask, render_template, jsonify, request
import json
from datetime import datetime, timedelta
import sqlite3
import logging
import os
import threading
import time
import asyncio
import queue
import hashlib

app = Flask(__name__)
app.secret_key = 'radar-secret-key-2025'

# –õ–æ–≥–∏
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
components_ready = False
collector = None
neural_analyzer = None
news_drafts = {}
initial_collection_done = False

# –û—á–µ—Ä–µ–¥–∏ –¥–ª—è —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
news_processing_queue = queue.Queue()
processing_results = {}
background_processor = None

def initialize_components():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã"""
    global components_ready, collector, neural_analyzer, background_processor
    
    try:
        logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã...")
        
        #  —Å–æ–∑–¥–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        setup_database()
        
        
        from data_collector import AdvancedFinanceNewsCollector
        collector = AdvancedFinanceNewsCollector()
        logger.info("‚úÖ –ö–æ–ª–ª–µ–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        def init_neural_in_background():
            global neural_analyzer
            try:
                from neural_analyzer import NeuralNewsAnalyzer
                neural_analyzer = NeuralNewsAnalyzer()
                neural_status = neural_analyzer.get_models_status()
                logger.info(f"üß† –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {neural_status}")
                
                if neural_analyzer.models_loaded:
                    start_background_processor()
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: {e}")
                neural_analyzer = None
        
        neural_thread = threading.Thread(target=init_neural_in_background)
        neural_thread.daemon = True
        neural_thread.start()
        
        # –°—Ä–∞–∑—É —Å–æ–∑–¥–∞–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ
        create_demo_data_if_needed()
        
        components_ready = True
        logger.info("‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {e}")
        components_ready = False

def start_background_processor():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    global background_processor
    
    if background_processor and background_processor.is_alive():
        return
    
    def process_news_background():
        """–§–æ–Ω–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏"""
        while True:
            try:
                # –ë–µ—Ä–µ–º –Ω–æ–≤–æ—Å—Ç—å –∏–∑ –æ—á–µ—Ä–µ–¥–∏ (—Å —Ç–∞–π–º–∞—É—Ç–æ–º —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å)
                try:
                    news_data = news_processing_queue.get(timeout=1.0)
                    news_id, article = news_data
                    
                    logger.info(f"üß† –§–æ–Ω–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏: {article['title'][:50]}...")
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é
                    if neural_analyzer and neural_analyzer.models_loaded:
                        processed_articles = asyncio.run(
                            neural_analyzer.process_articles_batch([article])
                        )
                        
                        if processed_articles:
                            enhanced_article = processed_articles[0]
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                            processing_results[news_id] = {
                                'enhanced_data': enhanced_article,
                                'processed_at': datetime.now(),
                                'status': 'completed'
                            }
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                            update_article_with_ai_data(news_id, enhanced_article)
                            
                            logger.info(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é: {news_id}")
                    
                    news_processing_queue.task_done()
                    
                except queue.Empty:
                    # –û—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∂–¥–∞—Ç—å
                    time.sleep(0.1)
                    continue
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                time.sleep(1)
    
    background_processor = threading.Thread(target=process_news_background)
    background_processor.daemon = True
    background_processor.start()
    logger.info("‚úÖ –§–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –∑–∞–ø—É—â–µ–Ω")

def update_article_with_ai_data(article_id, enhanced_data):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—å—é –≤ –±–∞–∑–µ —Å AI-–¥–∞–Ω–Ω—ã–º–∏"""
    try:
        conn = sqlite3.connect('data/news.db')
        cursor = conn.cursor()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è –ê–ò –¥–∞–Ω–Ω—ã—Ö
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ai_article_data (
                article_id TEXT PRIMARY KEY,
                enhanced_data TEXT,
                processed_at TIMESTAMP,
                ai_enhanced BOOLEAN DEFAULT 1
            )
        ''')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        cursor.execute('''
            INSERT OR REPLACE INTO ai_article_data 
            (article_id, enhanced_data, processed_at, ai_enhanced)
            VALUES (?, ?, ?, ?)
        ''', (
            article_id,
            json.dumps(enhanced_data, ensure_ascii=False),
            datetime.now().isoformat(),
            True
        ))
        
        conn.commit()
        conn.close()
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è AI-–¥–∞–Ω–Ω—ã—Ö: {e}")

def get_ai_enhanced_data(article_id):
    """–ü–æ–ª—É—á–∞–µ—Ç AI-—É–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç–∞—Ç—å–∏"""
    try:
        conn = sqlite3.connect('data/news.db')
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT enhanced_data FROM ai_article_data 
            WHERE article_id = ? AND ai_enhanced = 1
        ''', (article_id,))
        
        result = cursor.fetchone()
        conn.close()
        
        if result:
            return json.loads(result[0])
        return None
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è AI-–¥–∞–Ω–Ω—ã—Ö: {e}")
        return None

def setup_database():
    """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
    try:
        os.makedirs('data', exist_ok=True)
        
        conn = sqlite3.connect('data/news.db')
        cursor = conn.cursor()
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ 
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS raw_articles (
                id TEXT PRIMARY KEY,
                source_name TEXT,
                title TEXT,
                url TEXT,
                content TEXT,
                published_at TIMESTAMP,
                collected_at TIMESTAMP,
                language TEXT,
                category TEXT,
                is_finance BOOLEAN DEFAULT 0,
                country TEXT DEFAULT 'unknown',
                importance_score REAL DEFAULT 0.5
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ai_article_data (
                article_id TEXT PRIMARY KEY,
                enhanced_data TEXT,
                processed_at TIMESTAMP,
                ai_enhanced BOOLEAN DEFAULT 1
            )
        ''')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        cursor.execute("PRAGMA table_info(raw_articles)")
        existing_columns = [column[1] for column in cursor.fetchall()]
        
        required_columns = ['country', 'importance_score']
        for column in required_columns:
            if column not in existing_columns:
                if column == 'country':
                    cursor.execute("ALTER TABLE raw_articles ADD COLUMN country TEXT DEFAULT 'unknown'")
                elif column == 'importance_score':
                    cursor.execute("ALTER TABLE raw_articles ADD COLUMN importance_score REAL DEFAULT 0.5")
                logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞: {column}")
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã 
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_finance_published 
            ON raw_articles(is_finance, published_at)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_ai_processed 
            ON ai_article_data(processed_at)
        ''')
        
        conn.commit()
        conn.close()
        logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")

def get_real_news_from_db(hours=24, limit=50, sort_by='hotness', priority_filter='all'):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –±–∞–∑—ã —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
    try:
        if not os.path.exists('data/news.db'):
            return []
            
        conn = sqlite3.connect('data/news.db')
        cursor = conn.cursor()
        
        since_time = datetime.now() - timedelta(hours=hours)
        #–ë–ê–ó–ê SQL –ó–ê–ü–†–û–°–ê 
        base_query = '''
            SELECT id, source_name, title, url, content, published_at, country, importance_score
            FROM raw_articles 
            WHERE is_finance = 1 AND published_at >= ?
        '''
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        params = [since_time.isoformat()]
        
        priority_conditions = {
            'high': 'importance_score > 0.7',
            'medium': 'importance_score BETWEEN 0.4 AND 0.7', 
            'low': 'importance_score < 0.4',
            'all': '1=1'
        }
        
        if priority_filter in priority_conditions:
            base_query += f' AND {priority_conditions[priority_filter]}'
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É
        sort_options = {
            'hotness': 'importance_score DESC',
            'date_new': 'published_at DESC',
            'date_old': 'published_at ASC',
            'source': 'source_name ASC'
        }
        
        order_by = sort_options.get(sort_by, 'importance_score DESC')
        base_query += f' ORDER BY {order_by}'
        
        # 
        base_query += ' LIMIT ?'
        params.append(limit)
        
        cursor.execute(base_query, params)
        
        articles = []
        for row in cursor.fetchall():
            articles.append({
                'id': row[0],
                'source_name': row[1],
                'title': row[2],
                'url': row[3],
                'content': row[4] or '',
                'published_at': datetime.fromisoformat(row[5]) if row[5] else datetime.now(),
                'country': row[6] or 'unknown',
                'importance_score': row[7] or 0.5,
                'collected_at': datetime.now()
            })
        
        conn.close()
        return articles
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –±–∞–∑—ã: {e}")
        return []

def quick_entity_extraction(title):
    """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    text_lower = title.lower()
    entities = []
    
    company_keywords = ['—Å–±–µ—Ä–±–∞–Ω–∫', '–≥–∞–∑–ø—Ä–æ–º', '—Ä–æ—Å–Ω–µ—Ñ—Ç—å', '–ª—É–∫–æ–π–ª', '–≤—Ç–±', '—è–Ω–¥–µ–∫—Å',
                       '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', '–∞–ª—å—Ñ–∞-–±–∞–Ω–∫', '–º–æ—Å–±–∏—Ä–∂–∞', '—Ü–±', '–º–∏–Ω—Ñ–∏–Ω']
    
    for company in company_keywords:
        if company in text_lower:
            entities.append(company.title())
    
    return entities[:4] or ['–§–∏–Ω–∞–Ω—Å—ã']

def quick_draft_generation(title, entities):
    """–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–Ω–æ–≤–∏–∫–∞"""
    main_entity = entities[0] if entities else '—Ä—ã–Ω–∫–∞'
    
    return {
        'title': f"–ê–Ω–∞–ª–∏–∑: {title}",
        'lead': f"–°–æ–±—ã—Ç–∏–µ –ø—Ä–∏–≤–ª–µ–∫–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞.",
        'bullets': [
            f"–°–æ–±—ã—Ç–∏–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç {main_entity}",
            "–¢—Ä–µ–±—É–µ—Ç—Å—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–∑–≤–∏—Ç–∏—è",
            "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–π"
        ],
        'quote': "–°–∏—Ç—É–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è - —Å–∏—Å—Ç–µ–º–∞",
        'category': 'finance',
        'generated_by_ai': False
    }

def quick_why_now(importance_score):
    """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏"""
    if importance_score > 0.7:
        return "üî• –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç"
    elif importance_score > 0.5:
        return "üìà –í–∞–∂–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ"
    else:
        return "üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∫ —Å–≤–µ–¥–µ–Ω–∏—é"

def quick_timeline(article):
    """–ë—ã—Å—Ç—Ä—ã–π —Ç–∞–π–º–ª–∞–π–Ω"""
    pub_time = article.get('published_at', datetime.now())
    return [
        f"{pub_time.strftime('%H:%M')} - –ü—É–±–ª–∏–∫–∞—Ü–∏—è",
        "–°–ª–µ–¥—É—é—â–∏–π —á–∞—Å - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ–∞–∫—Ü–∏–∏"
    ]

def quick_impact_level(importance_score):
    """–ë—ã—Å—Ç—Ä—ã–π —Ä–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è"""
    if importance_score > 0.7:
        return "–≤—ã—Å–æ–∫–∏–π"
    elif importance_score > 0.5:
        return "—Å—Ä–µ–¥–Ω–∏–π"
    else:
        return "–±–∞–∑–æ–≤—ã–π"

def create_fast_news_format(raw_articles):
    """–ë—ã—Å—Ç—Ä–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Ñ–æ–Ω–æ–≤–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è"""
    processed_news = []
    
    for article in raw_articles:
        try:
            # 
            ai_data = get_ai_enhanced_data(article['id'])
            
            if ai_data:
                #  AI-—É–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                processed_news.append(ai_data)
                continue
            
            # –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ 
            title = article['title']
            importance_score = article.get('importance_score', 0.5)
            
            # –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
            entities = quick_entity_extraction(title)
            
            # –ë—ã—Å—Ç—Ä—ã–π —á–µ—Ä–Ω–æ–≤–∏–∫
            draft = quick_draft_generation(title, entities)
            
            processed_article = {
                'id': article['id'][:12],
                'headline': title,
                'hotness': importance_score,
                'why_now': quick_why_now(importance_score),
                'entities': entities,
                'sources': [article['url']],
                'timeline': quick_timeline(article),
                'draft': draft,
                'category': 'finance',
                'impact_level': quick_impact_level(importance_score),
                'source': article['source_name'],
                'published_at': article.get('published_at', datetime.now()).isoformat(),
                'ai_enhanced': False,
                'neural_processing': 'pending'  
            }
            
            if neural_analyzer and neural_analyzer.models_loaded:
                news_processing_queue.put((article['id'], article))
                processed_article['neural_processing'] = 'queued'
            
            processed_news.append(processed_article)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
            continue
    
    return processed_news

def create_demo_data_if_needed():
    """–°–æ–∑–¥–∞–µ—Ç –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –±–∞–∑–∞ –ø—É—Å—Ç–∞"""
    try:
        raw_articles = get_real_news_from_db(24, 5)
        if not raw_articles:
            logger.info("üìù –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ...")
            create_sample_articles()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–µ–º–æ-–¥–∞–Ω–Ω—ã—Ö: {e}")
        create_sample_articles()

def create_sample_articles():
    """–°–æ–∑–¥–∞–µ—Ç –æ–±—Ä–∞–∑—Ü–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏"""
    try:
        conn = sqlite3.connect('data/news.db')
        cursor = conn.cursor()
        
        sample_articles = [
            {
                'title': 'üî• –°–†–û–ß–ù–û: –¶–ë –†–§ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É –¥–æ 18%',
                'source': '–†–ë–ö',
                'content': '–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫ –ø—Ä–∏–Ω—è–ª —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –æ –ø–æ–≤—ã—à–µ–Ω–∏–∏ –∫–ª—é—á–µ–≤–æ–π —Å—Ç–∞–≤–∫–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã.',
                'url': 'https://www.rbc.ru/finance/',
                'importance': 0.95
            },
            {
                'title': '–°–±–µ—Ä–±–∞–Ω–∫ –æ–±—ä—è–≤–ª—è–µ—Ç –æ —Ä–µ–∫–æ—Ä–¥–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏ –ø–æ –∏—Ç–æ–≥–∞–º –∫–≤–∞—Ä—Ç–∞–ª–∞',
                'source': '–†–ë–ö',
                'content': '–ö—Ä—É–ø–Ω–µ–π—à–∏–π –±–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –ø–æ–∫–∞–∑–∞–ª —Ä–æ—Å—Ç –ø—Ä–∏–±—ã–ª–∏ –Ω–∞ 25% –±–ª–∞–≥–æ–¥–∞—Ä—è —É–≤–µ–ª–∏—á–µ–Ω–∏—é –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è.',
                'url': 'https://www.rbc.ru/finance/',
                'importance': 0.8
            },
            {
                'title': '–†—É–±–ª—å —É–∫—Ä–µ–ø–∏–ª—Å—è –∫ –¥–æ–ª–ª–∞—Ä—É –Ω–∞ —Ñ–æ–Ω–µ —Ä–æ—Å—Ç–∞ —Ü–µ–Ω –Ω–∞ –Ω–µ—Ñ—Ç—å',
                'source': '–í–µ–¥–æ–º–æ—Å—Ç–∏', 
                'content': '–ö—É—Ä—Å —Ä—É–±–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—É—é –¥–∏–Ω–∞–º–∏–∫—É –±–ª–∞–≥–æ–¥–∞—Ä—è —É–∫—Ä–µ–ø–ª–µ–Ω–∏—é —Ü–µ–Ω –Ω–∞ —ç–Ω–µ—Ä–≥–æ–Ω–æ—Å–∏—Ç–µ–ª–∏.',
                'url': 'https://www.vedomosti.ru/finance',
                'importance': 0.7
            },
            {
                'title': '–ú–æ—Å–±–∏—Ä–∂–∞ –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã',
                'source': '–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç',
                'content': '–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –ª–∏–Ω–µ–π–∫—É –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤.',
                'url': 'https://www.kommersant.ru/finance',
                'importance': 0.6
            },
            {
                'title': '–ò–Ω–≤–µ—Å—Ç–æ—Ä—ã –∞–∫—Ç–∏–≤–Ω–æ –ø–æ–∫—É–ø–∞—é—Ç –∞–∫—Ü–∏–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π',
                'source': '–§–∏–Ω–∞–º',
                'content': '–†—ã–Ω–æ–∫ –∞–∫—Ü–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–æ—Å—Ç –≤ —Å–µ–∫—Ç–æ—Ä–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –Ω–∞ —Ñ–æ–Ω–µ –æ–ø—Ç–∏–º–∏—Å—Ç–∏—á–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤.',
                'url': 'https://www.finam.ru/analysis/',
                'importance': 0.5
            },
            {
                'title': '–ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –æ–±—Å—É–∂–¥–∞—é—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Ä—ã–Ω–∫–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏',
                'source': '–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏',
                'content': '–≠–∫—Å–ø–µ—Ä—Ç—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ç–µ–∫—É—â—É—é —Å–∏—Ç—É–∞—Ü–∏—é –Ω–∞ —Ä—ã–Ω–∫–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏.',
                'url': 'https://ria.ru/economy/',
                'importance': 0.4
            },
            {
                'title': '–ï–∂–µ–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∏–Ω—Ñ–ª—è—Ü–∏–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω –ú–∏–Ω—ç–∫–æ–Ω–æ–º—Ä–∞–∑–≤–∏—Ç–∏—è',
                'source': '–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å',
                'content': '–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ –æ—á–µ—Ä–µ–¥–Ω–æ–π –æ—Ç—á–µ—Ç –ø–æ –∏–Ω—Ñ–ª—è—Ü–∏–æ–Ω–Ω—ã–º –æ–∂–∏–¥–∞–Ω–∏—è–º.',
                'url': 'https://www.interfax.ru/business/',
                'importance': 0.3
            }
        ]
        
        for article in sample_articles:
            article_id = hashlib.md5(article['title'].encode()).hexdigest()
            
            cursor.execute("SELECT id FROM raw_articles WHERE id = ?", (article_id,))
            if not cursor.fetchone():
                cursor.execute('''
                    INSERT INTO raw_articles 
                    (id, source_name, title, url, content, published_at, collected_at, language, category, is_finance, country, importance_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    article_id,
                    article['source'],
                    article['title'],
                    article['url'],
                    article['content'],
                    datetime.now().isoformat(),
                    datetime.now().isoformat(),
                    'ru',
                    'finance',
                    1,
                    'russia',
                    article['importance']
                ))
        
        conn.commit()
        conn.close()
        logger.info("‚úÖ –î–µ–º–æ-–¥–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ —Å–æ–∑–¥–∞–Ω—ã!")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ–º–æ-–¥–∞–Ω–Ω—ã—Ö: {e}")

def startup_sequence():
    """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø—É—Å–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
    print("üöÄ –ó–∞–ø—É—Å–∫ RADAR PRO System...")
    print("‚ö° –ü–æ–ª–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å + —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É")
    print("=" * 60)
    
    os.makedirs('data', exist_ok=True)
    os.makedirs('config', exist_ok=True)
    
    initialize_components()
    
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∑–∞–ø—É—â–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
    print("üåê –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://localhost:5000")
    print("‚è∞ –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞:", datetime.now().strftime('%Y-%m-%d %H:%M'))
    print("=" * 60)

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/news')
def get_news():
    """API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
    try:
        hours = request.args.get('hours', 24, type=int)
        limit = request.args.get('limit', 20, type=int)
        sort_by = request.args.get('sort', 'hotness')
        priority_filter = request.args.get('priority', 'all')
        
        logger.info(f"üìä –ó–∞–ø—Ä–æ—Å –Ω–æ–≤–æ—Å—Ç–µ–π: sort={sort_by}, priority={priority_filter}")
        
        raw_articles = get_real_news_from_db(
            hours=hours, 
            limit=limit, 
            sort_by=sort_by, 
            priority_filter=priority_filter
        )
        processed_news = create_fast_news_format(raw_articles)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º
        priority_stats = {
            'high': len([n for n in processed_news if n['hotness'] > 0.7]),
            'medium': len([n for n in processed_news if 0.4 <= n['hotness'] <= 0.7]),
            'low': len([n for n in processed_news if n['hotness'] < 0.4])
        }
        
        if processed_news:
            return jsonify({
                "news": processed_news,
                "status": "success",
                "message": f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed_news)} –Ω–æ–≤–æ—Å—Ç–µ–π",
                "sources_count": len(set([n['source'] for n in processed_news])),
                "neural_enhanced": any(n.get('ai_enhanced', False) for n in processed_news),
                "neural_queued": any(n.get('neural_processing') == 'queued' for n in processed_news),
                "sorting": {
                    "current_sort": sort_by,
                    "current_priority": priority_filter,
                    "priority_stats": priority_stats
                }
            })
        else:
            return jsonify({
                "news": [],
                "status": "no_data", 
                "message": "–ù–æ–≤–æ—Å—Ç–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è...",
                "sources_count": 0,
                "neural_enhanced": False
            })
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        return jsonify({
            "news": [],
            "status": "error",
            "message": f"–û—à–∏–±–∫–∞: {str(e)}"
        })

@app.route('/api/stats')
def get_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
    try:
        raw_articles = get_real_news_from_db(24, 100)
        total_articles = len(raw_articles)
        
        priority_stats = {
            'high': len([a for a in raw_articles if a['importance_score'] > 0.7]),
            'medium': len([a for a in raw_articles if 0.4 <= a['importance_score'] <= 0.7]),
            'low': len([a for a in raw_articles if a['importance_score'] < 0.4])
        }
        
        ai_processed = 0
        try:
            conn = sqlite3.connect('data/news.db')
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM ai_article_data")
            result = cursor.fetchone()
            ai_processed = result[0] if result else 0
            conn.close()
        except:
            pass
        
        return jsonify({
            "total_articles": total_articles,
            "last_24h": total_articles,
            "sources_count": len(set([article['source_name'] for article in raw_articles])),
            "ai_processed": ai_processed,
            "neural_ready": neural_analyzer is not None and getattr(neural_analyzer, 'models_loaded', False),
            "queue_size": news_processing_queue.qsize(),
            "priority_stats": priority_stats
        })
        
    except Exception as e:
        return jsonify({
            "total_articles": 0,
            "last_24h": 0,
            "sources_count": 0,
            "ai_processed": 0,
            "neural_ready": False,
            "queue_size": 0,
            "priority_stats": {'high': 0, 'medium': 0, 'low': 0}
        })

@app.route('/api/collect-now', methods=['POST'])
def collect_now():
    """–ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    try:
        def collect_background():
            try:
                from data_collector import AdvancedFinanceNewsCollector
                collector = AdvancedFinanceNewsCollector()
                articles = asyncio.run(collector.collect_news_async(hours_back=24))
                logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞: {e}")
        
        thread = threading.Thread(target=collect_background)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            "status": "success",
            "message": "–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞–ø—É—â–µ–Ω! –ù–æ–≤–æ—Å—Ç–∏ –ø–æ—è–≤—è—Ç—Å—è —á–µ—Ä–µ–∑ 1-2 –º–∏–Ω—É—Ç—ã."
        })
        
    except Exception as e:
        return jsonify({
            "status": "error", 
            "message": f"–û—à–∏–±–∫–∞: {str(e)}"
        })

@app.route('/news/<news_id>')
def news_detail(news_id):
    """–î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
    try:
        # –ò—â–µ–º —Å—Ç–∞—Ç—å—é –≤ –±–∞–∑–µ
        conn = sqlite3.connect('data/news.db')
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT id, source_name, title, url, content, published_at
            FROM raw_articles WHERE id LIKE ? || '%'
        ''', (news_id,))
        
        result = cursor.fetchone()
        conn.close()
        
        if not result:
            return render_template('error.html', message="–ù–æ–≤–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"), 404
        
        article = {
            'id': result[0],
            'source_name': result[1],
            'title': result[2],
            'url': result[3],
            'content': result[4] or '',
            'published_at': result[5]
        }
        
        # –£–ª—É—á—à –ê–ò—à–∫–æ–π
        ai_data = get_ai_enhanced_data(article['id'])
        
        if ai_data:
            news_item = ai_data
        else:
            # –æ—Ç–æ–±—Ä–∞–∂
            processed = create_fast_news_format([article])
            if processed:
                news_item = processed[0]
            else:
                return render_template('error.html', message="–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏"), 500
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —á–µ—Ä–Ω–æ–≤–∏–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
        if news_id in news_drafts:
            news_item['draft'] = news_drafts[news_id]
            
        return render_template('news_detail.html', news=news_item)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–µ—Ç–∞–ª–µ–π –Ω–æ–≤–æ—Å—Ç–∏: {e}")
        return render_template('error.html', message="–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏"), 500

@app.route('/api/save-draft/<news_id>', methods=['POST'])
def save_draft(news_id):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–Ω–æ–≤–∏–∫–∞"""
    try:
        draft_data = request.json
        news_drafts[news_id] = draft_data
        
        return jsonify({
            "status": "success",
            "message": "–ß–µ—Ä–Ω–æ–≤–∏–∫ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω!",
            "news_id": news_id
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {str(e)}"
        }), 500

@app.route('/api/get-draft/<news_id>')
def get_draft(news_id):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —á–µ—Ä–Ω–æ–≤–∏–∫–∞"""
    if news_id in news_drafts:
        return jsonify(news_drafts[news_id])
    else:
        return jsonify({
            "title": "",
            "lead": "", 
            "bullets": [],
            "quote": ""
        })

@app.route('/api/system-status')
def system_status():
    """–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã"""
    return jsonify({
        "database_ready": os.path.exists('data/news.db'),
        "articles_count": len(get_real_news_from_db(24, 10)),
        "neural_ready": neural_analyzer is not None and getattr(neural_analyzer, 'models_loaded', False),
        "initial_collection": initial_collection_done,
        "collector_ready": components_ready,
        "status": "operational"
    })

@app.route('/api/neural-status')
def neural_status():
    """–°—Ç–∞—Ç—É—Å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π"""
    if not neural_analyzer:
        return jsonify({"neural_models_loaded": False})
    
    status = neural_analyzer.get_models_status()
    status['queue_size'] = news_processing_queue.qsize()
    status['processing_results'] = len(processing_results)
    return jsonify(status)

if __name__ == '__main__':
    startup_sequence()
    app.run(debug=True, port=5000, host='0.0.0.0', threaded=True)