import torch
import numpy as np
import logging
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import re
from datetime import datetime, timedelta
import asyncio

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NeuralNewsAnalyzer:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")
        
        self.models_loaded = False
        self.embedding_model = None
        self.sentiment_model = None
        self.text_generator = None
        self.ner_pipeline = None
        
        self._load_models()

    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")
            
            # 1. –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ª–µ–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è)
            self.embedding_model = SentenceTransformer(
                'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                device=self.device
            )
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

            # 2. –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            try:
                self.sentiment_model = pipeline(
                    "sentiment-analysis",
                    model="blanchefort/rubert-base-cased-sentiment",
                    device=0 if torch.cuda.is_available() else -1,
                )
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
                self.sentiment_model = None

            # 3. NER –º–æ–¥–µ–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
            try:
                self.ner_pipeline = pipeline(
                    "ner",
                    model="Davlan/bert-base-multilingual-cased-ner-hrl",
                    aggregation_strategy="simple",
                    device=0 if torch.cuda.is_available() else -1,
                )
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è NER –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å NER –º–æ–¥–µ–ª—å: {e}")
                self.ner_pipeline = None

            # 4. –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            try:
                self.text_generator = pipeline(
                    "text-generation",
                    model="sberbank-ai/rugpt3small_based_on_gpt2",
                    device=0 if torch.cuda.is_available() else -1,
                )
                logger.info("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞: {e}")
                self.text_generator = None

            self.models_loaded = True
            logger.info("üéâ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            self.models_loaded = False

    async def analyze_sentiment(self, text):
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        if not self.sentiment_model or not text:
            return await self._fallback_sentiment(text)
        
        try:
            result = self.sentiment_model(text[:512])[0]
            
            sentiment_map = {
                'POSITIVE': 'positive',
                'NEGATIVE': 'negative', 
                'NEUTRAL': 'neutral'
            }
            
            return {
                'sentiment': sentiment_map.get(result['label'], 'neutral'),
                'confidence': result['score'],
                'scores': {
                    'positive': result['score'] if result['label'] == 'POSITIVE' else 1 - result['score'],
                    'negative': result['score'] if result['label'] == 'NEGATIVE' else 1 - result['score'],
                    'neutral': result['score'] if result['label'] == 'NEUTRAL' else 1 - result['score']
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
            return await self._fallback_sentiment(text)

    async def _fallback_sentiment(self, text):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        if not text:
            return {'sentiment': 'neutral', 'confidence': 0.5, 'scores': {'negative': 0.33, 'neutral': 0.34, 'positive': 0.33}}
        
        text_lower = text.lower()
        positive_words = ['—Ä–æ—Å—Ç', '—É–≤–µ–ª–∏—á', '–ø—Ä–∏–±—ã–ª—å', '—É—Å–ø–µ—Ö', '–ø–æ–∑–∏—Ç–∏–≤', '–≤—ã–≥–æ–¥', '—É–ª—É—á—à']
        negative_words = ['–ø–∞–¥–µ–Ω–∏–µ', '—Å–Ω–∏–∂', '—É–±—ã—Ç–æ–∫', '–ø—Ä–æ–±–ª–µ–º', '—Ä–∏—Å–∫', '–∫—Ä–∏–∑–∏—Å', '—Å–ª–æ–∂–Ω']
        
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            return {'sentiment': 'positive', 'confidence': 0.7, 'scores': {'negative': 0.2, 'neutral': 0.3, 'positive': 0.5}}
        elif negative_count > positive_count:
            return {'sentiment': 'negative', 'confidence': 0.7, 'scores': {'negative': 0.5, 'neutral': 0.3, 'positive': 0.2}}
        else:
            return {'sentiment': 'neutral', 'confidence': 0.6, 'scores': {'negative': 0.3, 'neutral': 0.4, 'positive': 0.3}}

    async def extract_entities_ner(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é NER"""
        if not self.ner_pipeline or not text:
            return await self._fallback_entities(text)
        
        try:
            truncated_text = text[:1000]
            entities = self.ner_pipeline(truncated_text)
            
            organized_entities = {
                'organizations': [],
                'persons': [],
                'locations': [],
                'misc': []
            }
            
            for entity in entities:
                entity_text = entity['word'].strip()
                entity_type = entity['entity_group']
                
                if entity_type in ['ORG', 'B-ORG', 'I-ORG'] and entity_text not in organized_entities['organizations']:
                    organized_entities['organizations'].append(entity_text)
                elif entity_type in ['PER', 'B-PER', 'I-PER'] and entity_text not in organized_entities['persons']:
                    organized_entities['persons'].append(entity_text)
                elif entity_type in ['LOC', 'B-LOC', 'I-LOC'] and entity_text not in organized_entities['locations']:
                    organized_entities['locations'].append(entity_text)
                elif entity_text not in organized_entities['misc']:
                    organized_entities['misc'].append(entity_text)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É—â–Ω–æ—Å—Ç–µ–π
            for key in organized_entities:
                organized_entities[key] = organized_entities[key][:5]
                
            return organized_entities
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ NER –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            return await self._fallback_entities(text)

    async def _fallback_entities(self, text):
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        text_lower = text.lower()
        
        # –°–ª–æ–≤–∞—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        companies = ['—Å–±–µ—Ä–±–∞–Ω–∫', '–≥–∞–∑–ø—Ä–æ–º', '—Ä–æ—Å–Ω–µ—Ñ—Ç—å', '–ª—É–∫–æ–π–ª', '–≤—Ç–±', '—è–Ω–¥–µ–∫—Å', 
                    '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', '–º–æ—Å–±–∏—Ä–∂–∞', '–∞–ª—å—Ñ–∞-–±–∞–Ω–∫', '—Ü–±', '–º–∏–Ω—Ñ–∏–Ω', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ',
                    'apple', 'microsoft', 'google', 'amazon', 'tesla', 'meta']
        
        persons = ['–ø—É—Ç–∏–Ω', '–º–∏—à—É—Å—Ç–∏–Ω', '–Ω–∞–±–∏—É–ª–ª–∏–Ω–∞', '—Å–∏–ª—É–∞–Ω–æ–≤', '–≥—Ä–µ—Ñ', '–º–∏–ª–ª–µ—Ä',
                  'biden', 'trump', 'putin', 'macron', 'scholz']
        
        locations = ['–º–æ—Å–∫–≤–∞', '—Ä–æ—Å—Å–∏—è', '—Å—à–∞', '–µ–≤—Ä–æ–ø–∞', '–∫–∏—Ç–∞–π', '–ª–æ–Ω–¥–æ–Ω', '–Ω—å—é-–π–æ—Ä–∫',
                    'moscow', 'russia', 'usa', 'europe', 'china', 'london']
        
        found_companies = [comp.title() for comp in companies if comp in text_lower]
        found_persons = [pers.title() for pers in persons if pers in text_lower]
        found_locations = [loc.title() for loc in locations if loc in text_lower]

        # –ü–æ–∏—Å–∫ –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—É–º–º –∏ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        money_patterns = [
            r'(\d+[,.]?\d*)\s*(–º–ª—Ä–¥|–º–∏–ª–ª–∏–∞—Ä–¥|billion)',
            r'(\d+[,.]?\d*)\s*(–º–ª–Ω|–º–∏–ª–ª–∏–æ–Ω|million)',
            r'(\d+[,.]?\d*)\s*%',
            r'\$(\d+[,.]?\d*)',
            r'‚Ç¨(\d+[,.]?\d*)'
        ]
        
        money_entities = []
        for pattern in money_patterns:
            matches = re.finditer(pattern, text_lower)
            for match in matches:
                money_entities.append(match.group(0))

        return {
            'organizations': found_companies[:5],
            'persons': found_persons[:3],
            'locations': found_locations[:3],
            'money': money_entities[:3],
            'misc': []
        }

    async def analyze_importance(self, title, content, source_name):
        """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ç—å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π"""
        try:
            text = f"{title}. {content[:500]}"
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            sentiment_result = await self.analyze_sentiment(text)
            
            # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
            base_score = 0.3
            
            # –§–∞–∫—Ç–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            source_weights = {
                'reuters': 0.9, 'bloomberg': 0.95, 'financial times': 0.9,
                '—Ä–±–∫': 0.85, '–∫–æ–º–º–µ—Ä—Å–∞–Ω—Ç': 0.8, '–≤–µ–¥–æ–º–æ—Å—Ç–∏': 0.8,
                '—Ü–±': 1.0, 'ecb': 0.9, 'imf': 0.9, 'world bank': 0.9
            }
            
            for source, weight in source_weights.items():
                if source in source_name.lower():
                    base_score = weight
                    break
            
            # –§–∞–∫—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            sentiment_boost = {
                'positive': 0.1,
                'negative': 0.15,  # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ —á–∞—Å—Ç–æ –≤–∞–∂–Ω–µ–µ
                'neutral': 0.0
            }
            base_score += sentiment_boost.get(sentiment_result['sentiment'], 0)
            
            # –§–∞–∫—Ç–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            urgency_indicators = {
                '—Å—Ä–æ—á–Ω': 0.2, '—ç–∫—Å—Ç—Ä–µ–Ω': 0.3, '–∫—Ä–∏–∑–∏—Å': 0.25, '–≤–∞–∂–Ω': 0.15,
                'urgent': 0.2, 'breaking': 0.3, 'crisis': 0.25, 'important': 0.15,
                '—Å–∞–Ω–∫—Ü': 0.2, '—Å–∞–Ω–∫—Ü–∏–∏': 0.2, 'sanction': 0.2,
                '—Ü–±': 0.3, 'central bank': 0.3, 'fed': 0.3
            }
            
            text_lower = text.lower()
            for indicator, boost in urgency_indicators.items():
                if indicator in text_lower:
                    base_score += boost
                    break
            
            # –§–∞–∫—Ç–æ—Ä –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ —á–∞—Å—Ç–æ –≤–∞–∂–Ω–µ–µ)
            content_length = len(content)
            if content_length > 1000:
                base_score += 0.1
            elif content_length > 500:
                base_score += 0.05
            
            final_score = min(max(base_score, 0.1), 0.99)
            
            logger.debug(f"üìä –û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ '{title[:30]}...': {final_score:.3f}")
            return final_score
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏: {e}")
            return await self._fallback_importance_analysis(title, content)

    async def _fallback_importance_analysis(self, title, content):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏"""
        text = f"{title} {content}".lower()
        score = 0.3
        
        important_indicators = [
            '—Å—Ä–æ—á–Ω', '—ç–∫—Å—Ç—Ä–µ–Ω', '–∫—Ä–∏–∑–∏—Å', '–≤–∞–∂–Ω', '–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω',
            'urgent', 'breaking', 'crisis', 'important', 'significant',
            '—Å–∞–Ω–∫—Ü', '—Ü–±', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–º–∏–Ω—Ñ–∏–Ω'
        ]
        
        for indicator in important_indicators:
            if indicator in text:
                score += 0.1
                
        return min(max(score, 0.1), 0.8)

    async def generate_ai_draft(self, article, entities, importance_score):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–Ω–æ–≤–∏–∫–∞ —Å –ø–æ–º–æ—â—å—é AI"""
        if not self.text_generator:
            return await self._generate_fallback_draft(article, entities)
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            context = self._prepare_generation_context(article, entities, importance_score)
            
            prompt = f"""
            –ù–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–∏–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ–±–∑–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:
            
            –ó–∞–≥–æ–ª–æ–≤–æ–∫: {article['title']}
            –û—Å–Ω–æ–≤–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {', '.join(entities.get('organizations', [])[:3])}
            
            –°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±–∑–æ—Ä:
            - –ö—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
            - –û—Å–Ω–æ–≤–Ω–æ–π –∞–±–∑–∞—Ü (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) —Å –∫–ª—é—á–µ–≤—ã–º–∏ –≤—ã–≤–æ–¥–∞–º–∏
            - 3 –∫–ª—é—á–µ–≤—ã—Ö –ø—É–Ω–∫—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞
            - –ó–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—É—é –º—ã—Å–ª—å –∏–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
            
            –¢–æ–Ω: –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π
            """
            
            result = self.text_generator(
                prompt,
                max_length=400,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=50256
            )
            
            generated_text = result[0]['generated_text']
            return self._parse_generated_draft(generated_text, article, entities)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI —á–µ—Ä–Ω–æ–≤–∏–∫–∞: {e}")
            return await self._generate_fallback_draft(article, entities)

    def _prepare_generation_context(self, article, entities, importance_score):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        context_parts = [
            f"–°—Ç–∞—Ç—å—è –æ—Ç {article.get('source_name', '–∏—Å—Ç–æ—á–Ω–∏–∫')}",
            f"–¢–µ–º–∞: {self._detect_topic(article['title'])}",
            f"–£—Ä–æ–≤–µ–Ω—å –≤–∞–∂–Ω–æ—Å—Ç–∏: {'–≤—ã—Å–æ–∫–∏–π' if importance_score > 0.7 else '—Å—Ä–µ–¥–Ω–∏–π' if importance_score > 0.4 else '–±–∞–∑–æ–≤—ã–π'}",
            f"–ö–ª—é—á–µ–≤—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {', '.join(entities.get('organizations', [])[:2])}"
        ]
        
        return ". ".join(context_parts)

    def _detect_topic(self, title):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º—ã —Å—Ç–∞—Ç—å–∏ —Å –ø–æ–º–æ—â—å—é AI"""
        title_lower = title.lower()
        
        topic_keywords = {
            'stocks': ['–∞–∫—Ü–∏', 'stock', 'equity', 'shares', '–±–∏—Ä–∂', 's&p', 'dow', 'nasdaq'],
            'bonds': ['–æ–±–ª–∏–≥–∞—Ü', 'bond', 'debt', 'coupon', 'yield'],
            'currency': ['–∫—É—Ä—Å', 'currency', 'dollar', 'euro', '—Ä—É–±–ª', 'ruble', 'exchange rate'],
            'commodities': ['–Ω–µ—Ñ—Ç—å', 'oil', '–≥–∞–∑', 'gas', '–∑–æ–ª–æ—Ç', 'gold', '–º–µ—Ç–∞–ª–ª', 'metal'],
            'crypto': ['–∫—Ä–∏–ø—Ç–æ', 'crypto', '–±–∏—Ç–∫–æ–∏–Ω', 'bitcoin', '–±–ª–æ–∫—á–µ–π–Ω', 'blockchain'],
            'banking': ['–±–∞–Ω–∫', 'bank', '–∫—Ä–µ–¥–∏—Ç', 'credit', '—Å—Ç–∞–≤–∫', 'interest rate'],
            'regulation': ['—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω', 'regulation', '—Å–∞–Ω–∫—Ü', 'sanction', '—Ü–±', 'central bank'],
            'macro': ['–≤–≤–ø', 'gdp', '–∏–Ω—Ñ–ª—è—Ü', 'inflation', '—ç–∫–æ–Ω–æ–º–∏–∫', 'economy']
        }
        
        for topic, keywords in topic_keywords.items():
            if any(keyword in title_lower for keyword in keywords):
                return topic
        
        return 'finance'

    def _parse_generated_draft(self, generated_text, article, entities):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        try:
            lines = [line.strip() for line in generated_text.split('\n') if line.strip()]
            
            if not lines or len(lines) < 3:
                return self._generate_fallback_draft(article, entities)

            draft = {
                'title': lines[0] if lines else f"–ê–Ω–∞–ª–∏–∑: {article['title']}",
                'lead': "",
                'bullets': [],
                'quote': "",
                'category': self._detect_topic(article['title']),
                'generated_by_ai': True,
                'ai_confidence': 0.8
            }
            
            # –£–º–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            current_section = 'lead'
            for line in lines[1:]:
                line_clean = line.strip()
                
                if not line_clean:
                    continue
                    
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º
                if any(marker in line_clean.lower() for marker in ['‚Ä¢', '- ', '‚Äî', '1.', '2.', '3.']):
                    if len(draft['bullets']) < 3:
                        # –û—á–∏—â–∞–µ–º –æ—Ç –º–∞—Ä–∫–µ—Ä–æ–≤
                        clean_bullet = re.sub(r'^[‚Ä¢\-\‚Äî\d\.\s]+', '', line_clean)
                        if clean_bullet and len(clean_bullet) > 10:
                            draft['bullets'].append(clean_bullet)
                elif len(line_clean) > 30 and not draft['lead']:
                    draft['lead'] = line_clean
                elif len(line_clean) > 20 and not draft['quote'] and any(marker in line_clean for marker in ['"', "'", '‚Äî']):
                    draft['quote'] = line_clean
                elif len(line_clean) > 15 and not draft['lead']:
                    draft['lead'] = line_clean

            # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —á–∞—Å—Ç–∏ —É–º–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if not draft['lead']:
                main_org = entities.get('organizations', ['–∫–æ–º–ø–∞–Ω–∏–π'])[0] if entities.get('organizations') else '—Ä—ã–Ω–∫–∞'
                draft['lead'] = f"–°–æ–±—ã—Ç–∏–µ {article['title']} –æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ {main_org} –∏ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤."
            
            if not draft['bullets']:
                main_org = entities.get('organizations', ['–∫–æ–º–ø–∞–Ω–∏–π'])[0] if entities.get('organizations') else '—Ä—ã–Ω–æ–∫'
                draft['bullets'] = [
                    f"–°–æ–±—ã—Ç–∏–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {main_org}",
                    "–ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Å–º–µ–∂–Ω—ã–µ —Å–µ–∫—Ç–æ—Ä–∞ —ç–∫–æ–Ω–æ–º–∏–∫–∏",
                    "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–∏—Ç—É–∞—Ü–∏–∏"
                ]
                
            if not draft['quote']:
                draft['quote'] = "–¢–µ–∫—É—â–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ —Ç—Ä–µ–±—É–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –º–æ–∂–µ—Ç –æ–∫–∞–∑–∞—Ç—å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏."
            
            return draft
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
            return self._generate_fallback_draft(article, entities)

    async def _generate_fallback_draft(self, article, entities):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–Ω–æ–≤–∏–∫–∞"""
        main_org = entities.get('organizations', ['–∫–æ–º–ø–∞–Ω–∏–π'])[0] if entities.get('organizations') else '—Ä—ã–Ω–∫–∞'
        
        return {
            'title': f"–ê–Ω–∞–ª–∏–∑: {article['title']}",
            'lead': f"–†–∞–∑–≤–∏—Ç–∏–µ —Å–∏—Ç—É–∞—Ü–∏–∏ –≤–æ–∫—Ä—É–≥ {main_org} —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞. –°–æ–±—ã—Ç–∏–µ –º–æ–∂–µ—Ç –æ–∫–∞–∑–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä—ã–Ω–æ—á–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏.",
            'bullets': [
                f"–°–æ–±—ã—Ç–∏–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å {main_org} –∏ —Å–º–µ–∂–Ω—ã–µ —Å–µ–∫—Ç–æ—Ä–∞ —ç–∫–æ–Ω–æ–º–∏–∫–∏",
                "–†—ã–Ω–æ—á–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –º–æ–∂–µ—Ç –æ–∫–∞–∑–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤",
                "–≠–∫—Å–ø–µ—Ä—Ç—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —Å–ª–µ–¥–∏—Ç—å –∑–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ–º —Å–æ–±—ã—Ç–∏–π –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è"
            ],
            'quote': "–¢–µ–∫—É—â–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ —Ç—Ä–µ–±—É–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ - —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —ç–∫—Å–ø–µ—Ä—Ç",
            'category': 'finance',
            'generated_by_ai': False,
            'ai_confidence': 0.0
        }

    async def process_articles_batch(self, articles):
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π"""
        if not articles:
            return []

        logger.info(f"üß† –ù–∞—á–∏–Ω–∞–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(articles)} —Å—Ç–∞—Ç–µ–π...")
        
        processed_articles = []
        
        for article in articles:
            try:
                full_text = f"{article['title']} {article.get('content', '')}"
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é NER
                entities = await self.extract_entities_ner(full_text)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
                importance = await self.analyze_importance(
                    article['title'],
                    article.get('content', ''),
                    article['source_name']
                )
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
                sentiment = await self.analyze_sentiment(full_text)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º AI —á–µ—Ä–Ω–æ–≤–∏–∫
                draft = await self.generate_ai_draft(article, entities, importance)
                
                # –°–æ–∑–¥–∞–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é
                processed_article = {
                    'id': article['id'],
                    'headline': article['title'],
                    'hotness': importance,
                    'why_now': self._generate_ai_why_now(importance, entities, sentiment),
                    'entities': self._flatten_entities(entities),
                    'sources': [article['url']],
                    'timeline': self._generate_ai_timeline(article, importance),
                    'draft': draft,
                    'category': draft['category'],
                    'impact_level': self._calculate_ai_impact_level(importance, entities, sentiment),
                    'source': article['source_name'],
                    'published_at': article.get('published_at', datetime.now()).isoformat(),
                    'ai_enhanced': True,
                    'sentiment': sentiment,
                    'ner_entities': entities
                }
                
                processed_articles.append(processed_article)
                logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —Å—Ç–∞—Ç—å—è: {article['title'][:50]}...")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
                continue

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        processed_articles.sort(key=lambda x: x['hotness'], reverse=True)
        
        logger.info(f"üéâ –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(processed_articles)} —Å—Ç–∞—Ç–µ–π")
        return processed_articles

    def _flatten_entities(self, entities):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫"""
        all_entities = []
        for entity_type, entity_list in entities.items():
            if entity_type != 'money':  # –ò—Å–∫–ª—é—á–∞–µ–º –¥–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã –∏–∑ –æ–±—â–µ–≥–æ —Å–ø–∏—Å–∫–∞
                all_entities.extend(entity_list[:2])
        return list(set(all_entities))[:6]

    def _generate_ai_why_now(self, importance, entities, sentiment):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é AI –ª–æ–≥–∏–∫–∏"""
        if importance > 0.8:
            orgs = entities.get('organizations', [])
            if orgs:
                return f"üî• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –í–ê–ñ–ù–û–°–¢–¨: –°–æ–±—ã—Ç–∏–µ –º–æ–∂–µ—Ç –æ–∫–∞–∑–∞—Ç—å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ {orgs[0]} –∏ —Ä—ã–Ω–æ–∫ –≤ —Ü–µ–ª–æ–º"
            return "üî• –í–´–°–û–ö–ê–Ø –°–†–û–ß–ù–û–°–¢–¨: –¢—Ä–µ–±—É–µ—Ç—Å—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–≤"
        elif importance > 0.6:
            if sentiment['sentiment'] == 'negative':
                return "‚ö†Ô∏è –ó–ù–ê–ß–ò–¢–ï–õ–¨–ù–´–ô –†–ò–°–ö: –ù–µ–≥–∞—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ —Ç—Ä–µ–±—É–µ—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–π"
            else:
                return "üìà –í–ê–ñ–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø: –ú–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –≤ —Å—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ"
        else:
            return "üìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ö –°–í–ï–î–ï–ù–ò–Æ: –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–∏—Ç—É–∞—Ü–∏–∏"

    def _generate_ai_timeline(self, article, importance):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–∞–π–º–ª–∞–π–Ω–∞ —Å AI –ª–æ–≥–∏–∫–æ–π"""
        pub_time = article.get('published_at', datetime.now())
        if isinstance(pub_time, str):
            try:
                pub_time = datetime.fromisoformat(pub_time.replace('Z', '+00:00'))
            except:
                pub_time = datetime.now()
        
        time_format = '%H:%M'
        
        base_timeline = [
            f"{pub_time.strftime(time_format)} - –ü—É–±–ª–∏–∫–∞—Ü–∏—è –≤ {article['source_name']}",
            f"{(pub_time + timedelta(minutes=30)).strftime(time_format)} - –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–∞—Ö"
        ]
        
        if importance > 0.7:
            base_timeline.extend([
                f"{(pub_time + timedelta(hours=1)).strftime(time_format)} - –ù–∞—á–∞–ª–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—Å—É–∂–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏",
                f"{(pub_time + timedelta(hours=2)).strftime(time_format)} - –û–∂–∏–¥–∞–µ—Ç—Å—è —Ä–µ–∞–∫—Ü–∏—è —Ä—ã–Ω–∫–∞"
            ])
        else:
            base_timeline.append(
                f"{(pub_time + timedelta(hours=1)).strftime(time_format)} - –ù–∞—á–∞–ª–æ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –æ–±—Å—É–∂–¥–µ–Ω–∏—è"
            )
        
        return base_timeline

    def _calculate_ai_impact_level(self, importance, entities, sentiment):
        """–†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è —Å AI –ª–æ–≥–∏–∫–æ–π"""
        important_entities = ['—Å–±–µ—Ä–±–∞–Ω–∫', '–≥–∞–∑–ø—Ä–æ–º', '—Ä–æ—Å–Ω–µ—Ñ—Ç—å', '–º–æ—Å–±–∏—Ä–∂–∞', '—Ü–±', '–º–∏–Ω—Ñ–∏–Ω',
                             'apple', 'microsoft', 'google', 'fed', 'ecb']
        has_important_entities = any(
            any(ent in entity.lower() for ent in important_entities) 
            for entity in entities.get('organizations', [])
        )
        
        sentiment_boost = 0.1 if sentiment['sentiment'] == 'negative' else 0
        
        adjusted_importance = importance + sentiment_boost
        
        if adjusted_importance > 0.7 and has_important_entities:
            return "–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π"
        elif adjusted_importance > 0.7:
            return "–≤—ã—Å–æ–∫–∏–π"
        elif adjusted_importance > 0.5:
            return "—Å—Ä–µ–¥–Ω–∏–π"
        else:
            return "–±–∞–∑–æ–≤—ã–π"

    def get_models_status(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return {
            'models_loaded': self.models_loaded,
            'embedding_model': self.embedding_model is not None,
            'sentiment_model': self.sentiment_model is not None,
            'text_generator': self.text_generator is not None,
            'ner_pipeline': self.ner_pipeline is not None,
            'device': str(self.device)
        }