import json
import requests
import sqlite3
import os
from datetime import datetime, timedelta
import time
import logging
from bs4 import BeautifulSoup
import hashlib
import re
import urllib3
import feedparser
import asyncio
import aiohttp
from urllib.parse import urljoin
import random

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedFinanceNewsCollector:
    def __init__(self, config_path="config/sources.json", db_path="data/news.db"):
        self.config_path = config_path
        self.db_path = db_path
        self.sources_config = self._load_config()
        self.session = None
        self._setup_directories()
        self._setup_database()

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö
        self.finance_keywords = [
            # Russian
            '—Ñ–∏–Ω–∞–Ω—Å', '—ç–∫–æ–Ω–æ–º–∏–∫', '–±–∏–∑–Ω–µ—Å', '—Ä—ã–Ω–æ–∫', '–∞–∫—Ü–∏', '–æ–±–ª–∏–≥–∞—Ü–∏', '–∏–Ω–≤–µ—Å—Ç', '–±–∞–Ω–∫',
            '–∫—É—Ä—Å', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '—Ä—É–±–ª', '–±–∏—Ä–∂', '—Ç—Ä–µ–π–¥', '–∫–∞–ø–∏—Ç–∞–ª', '–¥–∏–≤–∏–¥–µ–Ω–¥',
            '–ø—Ä–∏–±—ã–ª—å', '—É–±—ã—Ç–æ–∫', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '–∏–Ω—Ñ–ª—è—Ü', '–≤–≤–ø', 'IPO', 'SPO',
            '—Å–∞–Ω–∫—Ü', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫', '–º–µ—Ç–∞–ª–ª', '–∑–æ–ª–æ—Ç', '—Å–µ—Ä–µ–±—Ä',
            '–∫—Ä–∏–ø—Ç–æ', '–±–∏—Ç–∫–æ–∏–Ω', '–±–ª–æ–∫—á–µ–π–Ω', '–º–∞–π–Ω–∏–Ω–≥', '—Ç—Ä–µ–π–¥', '—Ç—Ä–µ–π–¥–µ—Ä',
            
            # English
            'finance', 'economy', 'business', 'market', 'stock', 'bond', 'investment', 'bank',
            'currency', 'dollar', 'euro', 'ruble', 'exchange', 'trade', 'capital', 'dividend',
            'profit', 'loss', 'budget', 'tax', 'inflation', 'gdp', 'IPO', 'offering',
            'sanction', 'oil', 'gas', 'energy', 'metal', 'gold', 'silver',
            'crypto', 'bitcoin', 'blockchain', 'mining', 'trader', 'trading'
        ]

        # User-Agents –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]

    def _load_config(self):
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª {self.config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return {"html_sources": [], "rss_sources": []}

    def _setup_directories(self):
        os.makedirs("data", exist_ok=True)
        os.makedirs("config", exist_ok=True)

    def _setup_database(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS raw_articles (
                    id TEXT PRIMARY KEY,
                    source_name TEXT,
                    title TEXT,
                    url TEXT,
                    content TEXT,
                    published_at TIMESTAMP,
                    collected_at TIMESTAMP,
                    language TEXT,
                    category TEXT,
                    is_finance BOOLEAN DEFAULT 0,
                    country TEXT DEFAULT 'unknown',
                    importance_score REAL DEFAULT 0.5
                )
            ''')
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_finance_published 
                ON raw_articles(is_finance, published_at)
            ''')
            
            conn.commit()
            conn.close()
            logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")

    def get_random_user_agent(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π User-Agent"""
        return random.choice(self.user_agents)

    async def collect_news_async(self, hours_back=48):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        logger.info("üöÄ –ó–ê–ü–£–°–ö –†–ê–°–®–ò–†–ï–ù–ù–û–ì–û –°–ë–û–†–ê –§–ò–ù–ê–ù–°–û–í–´–• –ù–û–í–û–°–¢–ï–ô")
        
        all_articles = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –∏–∑ HTML –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        logger.info("üìÑ –°–±–æ—Ä –∏–∑ HTML –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        html_articles = await self.parse_html_sources_async()
        all_articles.extend(html_articles)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        logger.info("üì° –°–±–æ—Ä –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        rss_articles = await self.parse_rss_sources_async()
        all_articles.extend(rss_articles)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –æ–±–æ–≥–∞—â–∞–µ–º —Å—Ç–∞—Ç—å–∏
        logger.info("üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π...")
        enriched_articles = await self.enrich_articles_async(all_articles)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
        saved_count = await self.save_to_database_async(enriched_articles)
        
        logger.info(f"‚úÖ –°–ë–û–† –ó–ê–í–ï–†–®–ï–ù. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(all_articles)}, –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö: {saved_count}")
        return enriched_articles

    async def parse_html_sources_async(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ HTML –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        articles = []
        semaphore = asyncio.Semaphore(5)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        
        async def process_source(source):
            async with semaphore:
                if source.get('type') != 'html':
                    return []
                
                try:
                    logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ HTML {source['name']}...")
                    
                    headers = {
                        'User-Agent': self.get_random_user_agent(),
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                    }
                    
                    async with aiohttp.ClientSession(headers=headers) as session:
                        async with session.get(source['url'], timeout=30, ssl=False) as response:
                            if response.status != 200:
                                logger.warning(f"–°—Ç–∞—Ç—É—Å {response.status} –¥–ª—è {source['name']}")
                                return []
                            
                            html = await response.text()
                            soup = BeautifulSoup(html, 'html.parser')
                            selectors = source.get('selectors', {})
                            
                            source_articles = []
                            article_selector = selectors.get('article', '')
                            
                            if article_selector:
                                article_elements = soup.select(article_selector)
                                logger.info(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ {source['name']}: {len(article_elements)}")
                                
                                for elem in article_elements[:15]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                                    article_data = self._extract_article_data(elem, selectors, source)
                                    if article_data:
                                        source_articles.append(article_data)
                            
                            # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                            await asyncio.sleep(1)
                            return source_articles
                            
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ HTML {source['name']}: {e}")
                    return []
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [process_source(source) for source in self.sources_config.get("html_sources", [])]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏
        for result in results:
            if isinstance(result, list):
                articles.extend(result)
        
        return articles

    async def parse_rss_sources_async(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö"""
        articles = []
        semaphore = asyncio.Semaphore(10)  # RSS –∑–∞–ø—Ä–æ—Å—ã –±—ã—Å—Ç—Ä–µ–µ, –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ
        
        async def process_source(source):
            async with semaphore:
                if source.get('type') != 'rss':
                    return []
                
                try:
                    logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ RSS {source['name']}...")
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º feedparser –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                    def parse_feed():
                        return feedparser.parse(source['url'])
                    
                    feed = await asyncio.get_event_loop().run_in_executor(None, parse_feed)
                    
                    if hasattr(feed, 'status') and feed.status != 200:
                        logger.warning(f"RSS —Å—Ç–∞—Ç—É—Å {feed.status} –¥–ª—è {source['name']}")
                        return []
                    
                    source_articles = []
                    entries = feed.entries[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ RSS —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ {source['name']}: {len(entries)}")
                    
                    for entry in entries:
                        article_data = self._extract_rss_article_data(entry, source)
                        if article_data:
                            source_articles.append(article_data)
                    
                    # –ö–æ—Ä–æ—Ç–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    await asyncio.sleep(0.5)
                    return source_articles
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ RSS {source['name']}: {e}")
                    return []
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [process_source(source) for source in self.sources_config.get("rss_sources", [])]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏
        for result in results:
            if isinstance(result, list):
                articles.extend(result)
        
        return articles

    def _extract_article_data(self, elem, selectors, source):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏ –∏–∑ HTML —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_elem = elem.select_one(selectors.get('title', '')) if selectors.get('title') else elem
            title = title_elem.get_text().strip() if title_elem else ''
            
            if not title or len(title) < 10:
                return None

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            link_elem = None
            if selectors.get('link'):
                link_elem = elem.select_one(selectors.get('link', ''))
            
            if not link_elem:
                link_elem = elem.find('a') if hasattr(elem, 'find') else None
            
            if not link_elem or not link_elem.get('href'):
                return None
            
            url = link_elem.get('href', '')
            url = self._parse_relative_url(url, source['url'])

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
            skip_keywords = ['facebook', 'twitter', 'instagram', 'vk.com', 'telegram', 
                           'youtube', 'login', 'signin', 'advertisement', 'ads']
            if any(keyword in url.lower() for keyword in skip_keywords):
                return None

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = ""
            if selectors.get('summary'):
                content_elem = elem.select_one(selectors.get('summary', ''))
                if content_elem:
                    content = content_elem.get_text().strip()

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
            published_at = datetime.now()
            if selectors.get('time'):
                time_elem = elem.select_one(selectors.get('time', ''))
                if time_elem:
                    time_text = time_elem.get_text().strip()
                    published_at = self._parse_time(time_text, source.get('language', 'ru'))

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –∏ —è–∑—ã–∫–∞
            language = source.get('language', self._detect_language(title))
            country = self._detect_country(source['name'], language)

            # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
            article_id = hashlib.md5(f"{title}{url}".encode()).hexdigest()
            
            article_data = {
                'id': article_id,
                'source_name': source['name'],
                'title': title,
                'url': url,
                'content': content,
                'published_at': published_at,
                'collected_at': datetime.now(),
                'language': language,
                'category': 'finance',
                'is_finance': True,
                'country': country,
                'importance_score': self._calculate_importance_score(title, content, source['name'])
            }
            
            return article_data
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏: {e}")
            return None

    def _extract_rss_article_data(self, entry, source):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ RSS —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            title = entry.get('title', '').strip()
            if not title:
                return None

            link = entry.get('link', '')
            if not link:
                return None

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
            content = ''
            if hasattr(entry, 'summary'):
                content = entry.summary
            elif hasattr(entry, 'description'):
                content = entry.description
            elif hasattr(entry, 'content'):
                if hasattr(entry.content[0], 'value'):
                    content = entry.content[0].value

            # –ü–∞—Ä—Å–∏–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published_at = datetime.now()
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_at = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                published_at = datetime(*entry.updated_parsed[:6])

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∏ —Å—Ç—Ä–∞–Ω—É
            language = source.get('language', self._detect_language(title))
            country = self._detect_country(source['name'], language)

            # –°–æ–∑–¥–∞–µ–º ID —Å—Ç–∞—Ç—å–∏
            article_id = hashlib.md5(f"{title}{link}".encode()).hexdigest()

            article_data = {
                'id': article_id,
                'source_name': source['name'],
                'title': title,
                'url': link,
                'content': content,
                'published_at': published_at,
                'collected_at': datetime.now(),
                'language': language,
                'category': 'finance',
                'is_finance': True,
                'country': country,
                'importance_score': self._calculate_importance_score(title, content, source['name'])
            }

            return article_data
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è RSS –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None

    def _parse_relative_url(self, url, base_url):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö URL"""
        if url.startswith('http'):
            return url
        elif url.startswith('//'):
            return f"https:{url}"
        elif url.startswith('/'):
            base = '/'.join(base_url.split('/')[:3])
            return base + url
        else:
            return urljoin(base_url, url)

    def _parse_time(self, time_text, language='ru'):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –¥–µ–º–æ - –≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –Ω—É–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å
            if 'hour' in time_text.lower() or '—á–∞—Å' in time_text.lower():
                hours = int(re.search(r'(\d+)', time_text).group(1))
                return datetime.now() - timedelta(hours=hours)
            elif 'minute' in time_text.lower() or '–º–∏–Ω—É—Ç' in time_text.lower():
                minutes = int(re.search(r'(\d+)', time_text).group(1))
                return datetime.now() - timedelta(minutes=minutes)
            elif 'day' in time_text.lower() or '–¥–µ–Ω—å' in time_text.lower() or '–¥–Ω' in time_text.lower():
                days = int(re.search(r'(\d+)', time_text).group(1))
                return datetime.now() - timedelta(days=days)
            else:
                return datetime.now()
        except:
            return datetime.now()

    def _detect_language(self, text):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return 'unknown'
        
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        cyrillic_count = sum(1 for char in text if '–∞' <= char <= '—è' or '–ê' <= char <= '–Ø')
        latin_count = sum(1 for char in text if 'a' <= char <= 'z' or 'A' <= char <= 'Z')
        
        if cyrillic_count > latin_count:
            return 'ru'
        elif latin_count > cyrillic_count:
            return 'en'
        else:
            return 'unknown'

    def _detect_country(self, source_name, language):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏ —è–∑—ã–∫–∞"""
        source_lower = source_name.lower()
        
        if any(word in source_lower for word in ['reuters', 'bloomberg', 'cnbc', 'financial times', 
                                               'marketwatch', 'yahoo', 'bbc', 'cnn', 'wall street']):
            return 'usa'
        elif any(word in source_lower for word in ['—Ä–±–∫', '–∫–æ–º–º–µ—Ä—Å–∞–Ω—Ç', '–≤–µ–¥–æ–º–æ—Å—Ç–∏', '—Ç–∞—Å—Å', 
                                                 '–∏–Ω—Ç–µ—Ä—Ñ–∞–∫—Å', '–ø—Ä–∞–π–º', '—Ñ–∏–Ω–∞–º', '–±–∞–Ω–∫–∏.—Ä—É']):
            return 'russia'
        elif language == 'ru':
            return 'russia'
        elif language == 'en':
            return 'usa'
        else:
            return 'international'

    def _calculate_importance_score(self, title, content, source_name):
        """–†–∞—Å—á–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ç—å–∏"""
        text = f"{title} {content}".lower()
        score = 0.3
        
        # –í–µ—Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        source_weights = {
            'reuters': 0.9, 'bloomberg': 0.95, 'financial times': 0.9,
            '—Ä–±–∫': 0.85, '–∫–æ–º–º–µ—Ä—Å–∞–Ω—Ç': 0.8, '–≤–µ–¥–æ–º–æ—Å—Ç–∏': 0.8,
            '—Ü–±': 1.0, 'ecb': 0.9, 'imf': 0.9, 'world bank': 0.9
        }
        
        for source, weight in source_weights.items():
            if source in source_name.lower():
                score = weight
                break
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏
        important_terms = {
            '—Å—Ä–æ—á–Ω': 0.2, '—ç–∫—Å—Ç—Ä–µ–Ω': 0.3, '–∫—Ä–∏–∑–∏—Å': 0.25, '–≤–∞–∂–Ω': 0.15,
            'urgent': 0.2, 'breaking': 0.3, 'crisis': 0.25, 'important': 0.15,
            '—Å–∞–Ω–∫—Ü': 0.2, '—Å–∞–Ω–∫—Ü–∏–∏': 0.2, 'sanction': 0.2,
            '—Ü–±': 0.3, 'central bank': 0.3, 'fed': 0.3,
            '–∫—É—Ä—Å': 0.15, 'exchange rate': 0.15, 'currency': 0.15,
            '–Ω–µ—Ñ—Ç—å': 0.2, 'oil': 0.2, '–≥–∞–∑': 0.2, 'gas': 0.2,
            '–±–∏—Ç–∫–æ–∏–Ω': 0.15, 'bitcoin': 0.15, '–∫—Ä–∏–ø—Ç–æ': 0.15, 'crypto': 0.15
        }
        
        for term, boost in important_terms.items():
            if term in text:
                score += boost
                break  # –¢–æ–ª—å–∫–æ –æ–¥–Ω–æ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ —Å–ª–æ–≤–æ
        
        return min(max(score, 0.1), 1.0)

    def _is_finance_article(self, title, content):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        text = (title + ' ' + content).lower()
        
        # –°—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        finance_matches = sum(1 for keyword in self.finance_keywords if keyword in text)
        
        # –ë–æ–ª–µ–µ –≥–∏–±–∫–∏–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        if finance_matches >= 1:  # –£–º–µ–Ω—å—à–∏–ª–∏ –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ—Ö–≤–∞—Ç–∞
            return True
            
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        specific_terms = ['ruble', 'rubl', 'mosprime', 'moex', 'rts', 'russian market']
        if any(term in text for term in specific_terms):
            return True
            
        return False

    async def enrich_articles_async(self, articles):
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        enriched = []
        
        for article in articles:
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                category = self._categorize_article(article['title'], article.get('content', ''))
                article['category'] = category
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏
                article['tags'] = self._extract_tags(article['title'], article.get('content', ''))
                
                # –£–ª—É—á—à–∞–µ–º –æ—Ü–µ–Ω–∫—É –≤–∞–∂–Ω–æ—Å—Ç–∏
                article['importance_score'] = self._calculate_importance_score(
                    article['title'], article.get('content', ''), article['source_name']
                )
                
                enriched.append(article)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
                enriched.append(article)  # –í—Å–µ —Ä–∞–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—å—é
        
        return enriched

    def _categorize_article(self, title, content):
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏"""
        text = (title + ' ' + content).lower()
        
        categories = {
            'stocks': ['–∞–∫—Ü–∏', 'stock', 'equity', 'shares', '–±–∏—Ä–∂', 's&p', 'dow', 'nasdaq'],
            'bonds': ['–æ–±–ª–∏–≥–∞—Ü', 'bond', 'debt', 'coupon', 'yield'],
            'currency': ['–∫—É—Ä—Å', 'currency', 'dollar', 'euro', '—Ä—É–±–ª', 'ruble', 'exchange rate'],
            'commodities': ['–Ω–µ—Ñ—Ç—å', 'oil', '–≥–∞–∑', 'gas', '–∑–æ–ª–æ—Ç', 'gold', '–º–µ—Ç–∞–ª–ª', 'metal'],
            'crypto': ['–∫—Ä–∏–ø—Ç–æ', 'crypto', '–±–∏—Ç–∫–æ–∏–Ω', 'bitcoin', '–±–ª–æ–∫—á–µ–π–Ω', 'blockchain'],
            'banking': ['–±–∞–Ω–∫', 'bank', '–∫—Ä–µ–¥–∏—Ç', 'credit', '—Å—Ç–∞–≤–∫', 'interest rate'],
            'regulation': ['—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω', 'regulation', '—Å–∞–Ω–∫—Ü', 'sanction', '—Ü–±', 'central bank'],
            'macro': ['–≤–≤–ø', 'gdp', '–∏–Ω—Ñ–ª—è—Ü', 'inflation', '—ç–∫–æ–Ω–æ–º–∏–∫', 'economy']
        }
        
        for category, keywords in categories.items():
            if any(keyword in text for keyword in keywords):
                return category
        
        return 'general'

    def _extract_tags(self, title, content):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏–∑ —Å—Ç–∞—Ç—å–∏"""
        text = (title + ' ' + content).lower()
        tags = set()
        
        # –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        entities = [
            '—Å–±–µ—Ä–±–∞–Ω–∫', 'sberbank', '–≥–∞–∑–ø—Ä–æ–º', 'gazprom', '—Ä–æ—Å–Ω–µ—Ñ—Ç—å', 'rosneft', 
            '–ª—É–∫–æ–π–ª', 'lukoil', '–≤—Ç–±', 'vtb', '—è–Ω–¥–µ–∫—Å', 'yandex', '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', 'tinkoff',
            'apple', 'microsoft', 'google', 'amazon', 'tesla', 'meta', 'facebook'
        ]
        
        for entity in entities:
            if entity in text:
                tags.add(entity)
        
        # –û–±—â–∏–µ —Ç–µ–º—ã
        themes = ['—Ä—ã–Ω–æ–∫', 'market', '–∏–Ω–≤–µ—Å—Ç', 'invest', '—Ç—Ä–µ–π–¥', 'trade', '—Ñ–∏–Ω–∞–Ω—Å', 'finance']
        for theme in themes:
            if theme in text:
                tags.add(theme)
        
        return list(tags)[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤

    async def save_to_database_async(self, articles):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        if not articles:
            logger.info("‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return 0

        try:
            def save_sync():
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                saved_count = 0
                
                for article in articles:
                    try:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –ª–∏ —ç—Ç–æ —Å—Ç–∞—Ç—å—è
                        is_finance = self._is_finance_article(article['title'], article.get('content', ''))
                        article['is_finance'] = is_finance
                        
                        cursor.execute('''
                            INSERT OR REPLACE INTO raw_articles
                            (id, source_name, title, url, content, published_at, collected_at, 
                             language, category, is_finance, country, importance_score)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            article['id'],
                            article['source_name'],
                            article['title'],
                            article['url'],
                            article['content'],
                            article['published_at'].isoformat(),
                            article['collected_at'].isoformat(),
                            article['language'],
                            article['category'],
                            article['is_finance'],
                            article.get('country', 'unknown'),
                            article.get('importance_score', 0.5)
                        ))
                        
                        if cursor.rowcount > 0:
                            saved_count += 1
                            if saved_count % 10 == 0:
                                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} —Å—Ç–∞—Ç–µ–π...")
                                
                    except Exception as e:
                        logger.error(f"‚ùå –û–®–ò–ë–ö–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø: {e}")
                        continue

                conn.commit()
                conn.close()
                return saved_count
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            saved_count = await asyncio.get_event_loop().run_in_executor(None, save_sync)
            logger.info(f"üíæ –í –ë–ê–ó–£ –°–û–•–†–ê–ù–ï–ù–û: {saved_count} —Å—Ç–∞—Ç–µ–π")
            return saved_count
            
        except Exception as e:
            logger.error(f"‚ùå –û–®–ò–ë–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•: {e}")
            return 0

    async def get_collection_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Å–±–æ—Ä—É"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            cursor.execute("SELECT COUNT(*) FROM raw_articles WHERE is_finance = 1")
            total_finance = cursor.fetchone()[0] or 0
            
            cursor.execute("SELECT COUNT(DISTINCT source_name) FROM raw_articles WHERE is_finance = 1")
            sources_count = cursor.fetchone()[0] or 0
            
            cursor.execute("SELECT COUNT(*) FROM raw_articles WHERE is_finance = 1 AND datetime(collected_at) > datetime('now', '-1 day')")
            last_24h = cursor.fetchone()[0] or 0
            
            cursor.execute("SELECT country, COUNT(*) FROM raw_articles WHERE is_finance = 1 GROUP BY country")
            countries_stats = cursor.fetchall()
            
            cursor.execute("SELECT language, COUNT(*) FROM raw_articles WHERE is_finance = 1 GROUP BY language")
            languages_stats = cursor.fetchall()
            
            conn.close()
            
            stats = {
                'total_finance_articles': total_finance,
                'sources_count': sources_count,
                'last_24h_articles': last_24h,
                'countries': dict(countries_stats),
                'languages': dict(languages_stats)
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
    async def run_collection():
        collector = AdvancedFinanceNewsCollector()
        
        print("üåê –ó–ê–ü–£–°–ö –†–ê–°–®–ò–†–ï–ù–ù–û–ì–û –°–ë–û–†–ê –ù–û–í–û–°–¢–ï–ô")
        print("=" * 60)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–±–æ—Ä
        articles = await collector.collect_news_async(hours_back=48)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = await collector.get_collection_stats()
        
        # –í—ã–≤–æ–¥–∏–º –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"\nüìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ë–û–†–ê")
        print("=" * 60)
        print(f"üìà –í—Å–µ–≥–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ –±–∞–∑–µ: {stats.get('total_finance_articles', 0)}")
        print(f"üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats.get('sources_count', 0)}")
        print(f"üïê –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞: {stats.get('last_24h_articles', 0)}")
        
        print(f"\nüåç –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –°–¢–†–ê–ù–ê–ú:")
        for country, count in stats.get('countries', {}).items():
            print(f"   {country}: {count} —Å—Ç–∞—Ç–µ–π")
            
        print(f"\nüó£Ô∏è –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –Ø–ó–´–ö–ê–ú:")
        for language, count in stats.get('languages', {}).items():
            print(f"   {language}: {count} —Å—Ç–∞—Ç–µ–π")
        
        if articles:
            print(f"\nüì∞ –ü–û–°–õ–ï–î–ù–ò–ï –§–ò–ù–ê–ù–°–û–í–´–ï –°–¢–ê–¢–¨–ò:")
            for i, article in enumerate(articles[:5], 1):
                print(f"\n{i}. [{article['source_name']}] [{article.get('country', 'unknown')}]")
                print(f"   üìù {article['title']}")
                print(f"   üîó {article['url']}")
                print(f"   ‚≠ê –í–∞–∂–Ω–æ—Å—Ç—å: {article.get('importance_score', 0.5):.2f}")
                if article.get('tags'):
                    print(f"   üè∑Ô∏è –¢–µ–≥–∏: {', '.join(article['tags'][:3])}")
        else:
            print("\n‚ùå –í —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ –Ω–µ —Å–æ–±—Ä–∞–Ω–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
            
        print(f"\n‚úÖ –°–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")

    asyncio.run(run_collection())

if __name__ == "__main__":
    main()